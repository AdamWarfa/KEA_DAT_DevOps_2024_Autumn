<div class="title-card" style="color: cyan;">
    <h1>Web crawling architectural decisions</h1>
</div>

---

# Possibility 1: Upload the entry as you go

Downsides: 

* Going row by row will be resource intensive on your production server.

* Challenge with looking up if the URL has already been visited in the production database.

* Won't work if your job can't connect to the server.

---

# Possibility 2: Use a database

1. In the spider connect to a database (SQLite for instance)

2. Store the data in the database

3. Query the database to check if the URL has already been visited

4. If the URL has not been visited, store the data in the database

---

# Possibility 2: Two options

## Option 1: Transfer the database as a file

If you are using SQLite, you can conveniently transfer the database as a file to the server. 

You can then have separate database between your web scraping and your other business logic.

## Option 2: Run a post script that uploads the data in batches

Benefit: Batch upload will be less resource intensive compared to uploading row by row as you crawl.

---

<div class="title-card" style="color: cyan;">
    <h1>Where to run the script?</h1>
</div>

---

# Cron jobs / Github Actions Schedule

### Cron job syntax:

https://crontab.guru/

^ Cronitor = Cron job monitoring

### Github Actions schedule:

https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule

* Jobs may be dropped during high load times. Avoid this by scheduling at odd hours.
* In a public repository, scheduled workflows are automatically disabled when no repository activity has occurred in 60 days. For information on re-enabling a disabled workflow, see "Disabling and enabling a workflow."

---


# Github Actions - Example


```yaml
name: Run Scrapy Project Daily

on:
  schedule:
    - cron: '0 0 * * *'  # Runs at midnight (UTC) every day

jobs:
  run-scrapy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Scrapy Spider
        run: |
          scrapy crawl <spider_name> -o output.json

```

---




# Serverless - Azure Functions



---

# Create a ressource group
az group create --name <RESOURCE_GROUP_NAME> --location <LOCATION>